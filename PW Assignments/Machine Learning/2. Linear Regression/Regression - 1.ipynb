{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4857c94b-7d81-4595-b9ad-4deb24ecc16f",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e926c08-20ab-4617-aebb-8e7b00a9d6e6",
   "metadata": {},
   "source": [
    "==> \n",
    "Simple linear regression: Predicts one variable (y) based on another variable (x).\n",
    "Example: Predicting house price based on square footage.\n",
    "\n",
    "Multiple linear regression: Predicts one variable (y) based on two or more variables (x1, x2, ...).\n",
    "Example: Predicting house price based on square footage, number of bedrooms, and location.\n",
    "\n",
    "In short, multiple linear regression is a more complex type of regression that can account for multiple factors that influence the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c909c9-27c8-4519-8cb5-38c0f51ee990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af3e14c6-abb4-465f-a24a-cd491cd2f8be",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204aa9fd-1c62-46f7-8415-e6dace61b805",
   "metadata": {},
   "source": [
    "==> \n",
    "Assumptions of linear regression:\n",
    "\n",
    "    1. Linear relationship between the dependent variable and independent variables.\n",
    "    2. Independent variables are independent of each other.\n",
    "    3. Homoscedasticity (constant variance of errors).\n",
    "    4. Normality of errors.\n",
    "    5. No outliers.\n",
    "How to check whether these assumptions hold in a given dataset:\n",
    "\n",
    "    1. Linear relationship: Plot the dependent variable against each independent variable to see if there is a linear trend.\n",
    "    2. Independence of independent variables: Calculate the correlation matrix between the independent variables and check for high correlations.\n",
    "    3. Homoscedasticity: Plot the residuals (difference between predicted and actual values) against the predicted values to see if the variance is constant.\n",
    "    4. Normality of errors: Plot a histogram of the residuals to see if it is approximately normal.\n",
    "    5. No outliers: Plot the residuals against the predicted values and look for any points that are far away from the rest of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40b42a-af6a-465b-9acf-64250d8f8669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80f3b799-fa39-42bb-a68d-7efcb29c8adf",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1b911-639c-4dfa-b257-47d440ed86a7",
   "metadata": {},
   "source": [
    "==> The slope and intercept of a linear regression model can be interpreted as follows:\n",
    "\n",
    "    1. Slope: The slope tells you how much the dependent variable changes for every unit change in the independent variable.\n",
    "    2. Intercept: The intercept tells you the predicted value of the dependent variable when the independent variable is equal to zero.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose we have a linear regression model to predict house price based on square footage. The slope of the model is 100, which means that for every additional square foot of living space, the predicted house price increases by $100. \n",
    "The intercept of the model is 50000, which means that if a house has zero square feet of living space, the predicted house price is $50000.\n",
    "\n",
    "In minimum line:\n",
    "\n",
    "    1. Slope: Change in dependent variable for every unit change in independent variable.\n",
    "    2. Intercept: Predicted value of dependent variable when independent variable is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc73c57-ff55-42cd-a339-fbed673d7bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "282df7ad-be9e-45f1-9772-88fb0567ed56",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf1eca-d11a-420c-91bf-4150d178037c",
   "metadata": {},
   "source": [
    "==> \n",
    "    1. Gradient descent is a method to find the minimum of a function, like finding the lowest point in a valley.\n",
    "    2. In machine learning, it's used to minimize a \"cost\" or \"error\" function by adjusting model parameters iteratively.\n",
    "    3. It calculates the direction (gradient) to move in and the size of each step to reach the minimum.\n",
    "\n",
    "In simple terms, it helps machine learning models learn by figuring out how to tweak their settings gradually to make better predictions or fit data more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef4319-5629-4a3d-b9fc-86a4eb94ba97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81787730-b53b-48f9-a0a4-57f59defaab9",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796be0ef-a60a-4215-bd68-f616ea0fa07b",
   "metadata": {},
   "source": [
    "==> Multiple linear regression is a statistical model that predicts one variable (y) based on two or more variables (x1, x2, ...). It is an extension of simple linear regression, which predicts one variable based on another variable.\n",
    "\n",
    "The difference between simple linear regression and multiple linear regression is that multiple linear regression can account for multiple factors that influence the dependent variable.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose we want to predict house price. We could use simple linear regression to predict house price based on square footage. However, house price is also influenced by other factors, such as number of bedrooms, location, and school district. By using multiple linear regression, we can account for all of these factors to get a more accurate prediction of house price.\n",
    "\n",
    "In minimum line:\n",
    "\n",
    "Multiple linear regression: Predicts one variable (y) based on two or more variables (x1, x2, ...).\n",
    "Simple linear regression: Predicts one variable (y) based on another variable (x).\n",
    "\n",
    "Multiple linear regression is more complex than simple linear regression, but it can be more accurate when there are multiple factors that influence the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f82c5-b462-460e-85bb-9dc2459c4918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f99f7f31-5e3f-4c7e-9912-2a936e758a12",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ec1a5-b1f8-468c-8a09-df74ea698ca7",
   "metadata": {},
   "source": [
    "==> Multicollinearity in multiple linear regression is a problem that occurs when two or more independent variables are highly correlated with each other. This can make it difficult to interpret the results of the regression model, and it can also lead to inaccurate predictions.\n",
    "\n",
    "There are a few ways to detect multicollinearity in multiple linear regression:\n",
    "\n",
    "Correlation matrix: Calculate the correlation matrix between the independent variables. If any of the correlations are high (above 0.7), this could be a sign of multicollinearity.\n",
    "Variance inflation factor (VIF: Calculate the VIF for each independent variable. A VIF of 5 or higher is generally considered to be a sign of multicollinearity.\n",
    "\n",
    "If you detect multicollinearity in your regression model, there are a few things you can do to address the issue:\n",
    "\n",
    "Remove one or more of the independent variables: If two or more independent variables are highly correlated, you can remove one or more of them from the model.\n",
    "Combine the independent variables: You can also combine the independent variables into a single variable. This is most effective if the independent variables have a similar meaning.\n",
    "Use a different regression model: There are a number of regression models that are specifically designed to handle multicollinearity. These models can be more complex to use, but they can also be more accurate.\n",
    "\n",
    "In minimum line:\n",
    "\n",
    "Multicollinearity: Two or more independent variables are highly correlated with each other.\n",
    "Detection: Correlation matrix, variance inflation factor (VIF).\n",
    "Solutions: Remove variables, combine variables, use different regression model.\n",
    "\n",
    "It is important to note that multicollinearity is not always a problem. In some cases, it may be necessary to keep all of the independent variables in the model, even if they are correlated. It is important to weigh the costs and benefits of removing variables before making a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54606e-834d-4f22-8f45-299a5c2d3a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04ff96e0-0f69-4f65-893d-b2164b0da93f",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d9275-6712-4b4c-bb12-1f12c42e71d9",
   "metadata": {},
   "source": [
    "Polynomial Regression:\n",
    "- Polynomial regression is a type of regression model that uses polynomial equations to fit a curve to the data.\n",
    "- Unlike linear regression, which fits a straight line, polynomial regression can capture more complex, curved relationships between variables.\n",
    "\n",
    "Difference from Linear Regression:\n",
    "- Linear regression fits a straight line to data.\n",
    "- Polynomial regression fits curves, like quadratic (X^2), cubic (X^3), or higher-degree functions to better capture non-linear patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21451dd4-37e7-417e-8887-5d6d8ffe57fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b65f8d3c-0384-4d80-935c-8775a4f8dcbf",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c882f-9a78-4b82-ac55-b53d58bec27f",
   "metadata": {},
   "source": [
    "**Advantages of polynomial regression over linear regression:**\n",
    "\n",
    "* Can model non-linear relationships between variables.\n",
    "* Can often provide a better fit to the data than linear regression.\n",
    "\n",
    "**Disadvantages of polynomial regression over linear regression:**\n",
    "\n",
    "* More complex to implement and interpret.\n",
    "* More prone to overfitting the data.\n",
    "\n",
    "**When to use polynomial regression:**\n",
    "\n",
    "* When the relationship between the variables is non-linear.\n",
    "* When linear regression does not provide a good fit to the data.\n",
    "\n",
    "It is important to note that polynomial regression is more prone to overfitting than linear regression. Overfitting occurs when the model learns the noise in the training data too well, and is therefore unable to generalize to new data. To avoid overfitting, it is important to use a validation set to evaluate the model and to choose a polynomial degree that is not too high.\n",
    "\n",
    "Here are some examples of situations where you might prefer to use polynomial regression:\n",
    "\n",
    "* Predicting the growth of a child based on their age.\n",
    "* Predicting the demand for a product based on the price.\n",
    "* Predicting the trajectory of a projectile.\n",
    "\n",
    "In these cases, the relationship between the variables is non-linear, and linear regression would not provide a good fit to the data. Polynomial regression can be used to model these relationships more accurately.\n",
    "\n",
    "However, it is important to be aware of the potential for overfitting when using polynomial regression. It is important to use a validation set to evaluate the model and to choose a polynomial degree that is not too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b6db6-b09d-48c0-b0c8-002b712a26f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
