{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056fec9c",
   "metadata": {},
   "source": [
    "# Q1. Theory and Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc93fe",
   "metadata": {},
   "source": [
    "# Q1.1. What is batch normalization in the context of Artificial Neural Networks (ANN)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbbf4e",
   "metadata": {},
   "source": [
    "Answer: Batch normalization is a technique used in artificial neural networks to standardize the inputs of each layer. It involves normalizing the activations of each layer to have zero mean and unit variance, which helps in stabilizing and accelerating the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab051a3a",
   "metadata": {},
   "source": [
    "# Q1.2. What are the benefits of using batch normalization during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c7a16",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "\n",
    " -   Improved Training Speed: Batch normalization allows for higher learning rates, accelerating the training process.\n",
    " -   Stabilizes Learning: It mitigates the vanishing or exploding gradients problem, leading to more stable training.\n",
    " -   Regularization: Acts as a form of regularization, reducing the need for other techniques like dropout.\n",
    " -   Enhanced Generalization: It tends to improve the generalization ability of the model by reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcbb886",
   "metadata": {},
   "source": [
    "# Q1.3. How does batch normalization work, including the normalization step and learnable parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f6325",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "\n",
    "   - Normalization Step: In batch normalization, the activations of each layer are normalized by subtracting the mean and dividing by the standard deviation of the mini-batch.\n",
    "   - Learnable Parameters: Batch normalization introduces two learnable parameters, scale (γγ) and shift (ββ), which allow the model to learn the optimal scaling and shifting of the normalized activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b17623",
   "metadata": {},
   "source": [
    "# Q2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a11677",
   "metadata": {},
   "source": [
    "# Q2.1. Choose a dataset and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4868b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example preprocessing for MNIST dataset using TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d305e87",
   "metadata": {},
   "source": [
    "# Q2.2. Implement a simple feedforward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41ef819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAHUL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Example implementation of a simple feedforward neural network using TensorFlow\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b42964",
   "metadata": {},
   "source": [
    "# Q2.3. Train the neural network without using batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c348d922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8784 - loss: 0.4322 - val_accuracy: 0.9594 - val_loss: 0.1364\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.1213 - val_accuracy: 0.9690 - val_loss: 0.0993\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.0807 - val_accuracy: 0.9737 - val_loss: 0.0846\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9834 - loss: 0.0582 - val_accuracy: 0.9763 - val_loss: 0.0783\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9866 - loss: 0.0417 - val_accuracy: 0.9778 - val_loss: 0.0734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2340603a200>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5afabe",
   "metadata": {},
   "source": [
    "# Q2.4. Implement batch normalization layers in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "580590fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example implementation of a feedforward neural network with batch normalization using TensorFlow\n",
    "model_bn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c99599e",
   "metadata": {},
   "source": [
    "# Q2.5. Train the model again with batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28328ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.8846 - loss: 0.3938 - val_accuracy: 0.9613 - val_loss: 0.1330\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9605 - loss: 0.1318 - val_accuracy: 0.9721 - val_loss: 0.0940\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9722 - loss: 0.0929 - val_accuracy: 0.9718 - val_loss: 0.0912\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9779 - loss: 0.0733 - val_accuracy: 0.9757 - val_loss: 0.0821\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9813 - loss: 0.0607 - val_accuracy: 0.9756 - val_loss: 0.0746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23404529d50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bn.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model_bn.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab9364",
   "metadata": {},
   "source": [
    "# Q2.6. Compare the training and validation performance between the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbef8d",
   "metadata": {},
   "source": [
    "Based on the training log you provided, here are the validation performance metrics after training the model with batch normalization:\n",
    "\n",
    "    Validation Accuracy: 97.56%\n",
    "    Validation Loss: 0.0746\n",
    "\n",
    "Comparing this with the performance without batch normalization would give you a clear picture of the impact of batch normalization on the model's performance. If you need further analysis or comparison, you can calculate the difference in accuracy and loss between the two models to quantify the improvement brought by batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade299ed",
   "metadata": {},
   "source": [
    "# Q3. Experimentation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c956c9",
   "metadata": {},
   "source": [
    "# Q3.1. Experiment with different batch sizes and observe their effect on training dynamics and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019e321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9829 - loss: 0.0534 - val_accuracy: 0.9770 - val_loss: 0.0743\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9862 - loss: 0.0429 - val_accuracy: 0.9784 - val_loss: 0.0731\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9870 - loss: 0.0403 - val_accuracy: 0.9784 - val_loss: 0.0754\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9888 - loss: 0.0359 - val_accuracy: 0.9790 - val_loss: 0.0718\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9905 - loss: 0.0314 - val_accuracy: 0.9794 - val_loss: 0.0677\n",
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9952 - loss: 0.0169 - val_accuracy: 0.9808 - val_loss: 0.0678\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9963 - loss: 0.0135 - val_accuracy: 0.9783 - val_loss: 0.0737\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9968 - loss: 0.0121 - val_accuracy: 0.9798 - val_loss: 0.0687\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9975 - loss: 0.0103 - val_accuracy: 0.9792 - val_loss: 0.0710\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9977 - loss: 0.0091 - val_accuracy: 0.9792 - val_loss: 0.0723\n",
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9991 - loss: 0.0049 - val_accuracy: 0.9801 - val_loss: 0.0721\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0036 - val_accuracy: 0.9801 - val_loss: 0.0767\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0035 - val_accuracy: 0.9815 - val_loss: 0.0797\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0027 - val_accuracy: 0.9807 - val_loss: 0.0773\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0023 - val_accuracy: 0.9785 - val_loss: 0.0859\n"
     ]
    }
   ],
   "source": [
    "# Example training with different batch sizes using TensorFlow\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    model_bn.compile(optimizer='adam',\n",
    "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "    model_bn.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c702a0",
   "metadata": {},
   "source": [
    "# Q3.2. Discuss the advantages and limitations of batch normalization in improving the training of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8705370",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "    Accelerates training by allowing higher learning rates.\n",
    "    Stabilizes training by mitigating the vanishing or exploding gradients problem.\n",
    "    Acts as a form of regularization, reducing overfitting.\n",
    "    Enhances the generalization ability of the model.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "    Adds computational overhead during inference.\n",
    "    Sensitivity to batch size, which may affect performance.\n",
    "\n",
    "By conducting these experiments and analyses, you'll gain insights into how batch normalization affects training dynamics and model performance. Let me know if you need further assistance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccddb00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
